# Paper Review


## Out-of-Distribution
### Survey
**1. [arXiv'21]** [Generalized Out-of-Distribution Detection: A Survey](https://arxiv.org/abs/2110.11334)

<br/>

### Dataset
**1. [ICML'22]** [WILDS: A Benchmark of in-the-Wild Distribution Shifts](https://arxiv.org/abs/2012.07421)

<br/>

### Classification-based methods
**1. [NeurIPS'21]** [Exploring the Limits of Out-of-Distribution Detection](https://arxiv.org/abs/2106.03004)

**2. [ICLR'23]** [RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data](https://arxiv.org/abs/2302.14483)

**3. [NeurIPS'22]** [On Uncertainty, Tempering, and Data Augmentation in Bayesian Classification](https://arxiv.org/abs/2203.16481)

**4. [NeurIPS'21]** [Laplace Redux -- Effortless Bayesian Deep Learning](https://arxiv.org/abs/2106.14806)

<br/>

### Distance-based methods
**1. [NeurIPS'18]** [A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks](https://arxiv.org/abs/1807.03888)

<br/>

### Density-based methods
**1. [ICLR'19]** [Do Deep Generative Models Know What They Don't Know?](https://arxiv.org/abs/1810.09136)

**2. [NeurIPS'19]** [Likelihood Ratios for Out-of-Distribution Detection](https://arxiv.org/abs/1906.02845)

**3. [AISTATS'21]** [Density of States Estimation for Out-of-Distribution Detection](https://arxiv.org/abs/2006.09273)

<br/>

## Continual Lifelong Learning
### Survey
**1. [Neural Networks'19]** [Continual Lifelong Learning with Neural Networks: A Review](https://arxiv.org/abs/1802.07569)

<br/>

### Regularisation-based approach
**1. [ICCV'19]** [Overcoming Catastrophic Forgetting with Unlabeled Data in the Wild](https://arxiv.org/abs/1903.12648)

**2. [PNAS'17]** [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/1612.00796)

<br/>

### Replay-based approach
**1. [NeurIPS'17]** [Continual Learning with Deep Generative Replay](https://arxiv.org/abs/1705.08690)

<br/>

### Architecture-based approach
**1. [ICLR'18]** [Lifelong Learning with Dynamically Expandable Networks](https://arxiv.org/abs/1708.01547)

<br/>

## Self-Supervised Learning
### Survey
**1. [Technologies'20]** [A Survey on Contrastive Self-supervised Learning](https://arxiv.org/abs/2011.00362)

<br/>

### Contrastive methods
**1. [ICML'20]** [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)

**2. [CVPR'20]** [Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/1911.05722)

<br/>

### Knowledge distillation methods
**1. [NeurIPS'20]** [Bootstrap your own latent: A new approach to self-supervised Learning](https://arxiv.org/abs/2006.07733)

**2. [CVPR'21]** [Exploring Simple Siamese Representation Learning](https://arxiv.org/abs/2011.10566)

<br/>

### Information maximisation methods
**1. [ICML'21]** [Barlow Twins: Self-Supervised Learning via Redundancy Reduction](https://arxiv.org/abs/2103.03230)

**2. [ICLR'22]** [VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning](https://arxiv.org/abs/2105.04906)

**3. [NeurIPS'22]** [VICRegL: Self-Supervised Learning of Local Visual Features](https://arxiv.org/abs/2210.01571)

<br/>

## Computer Vision
### Image classification
**1. [CVPR'16]** [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)

